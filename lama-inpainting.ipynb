{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a68a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/advimman/lama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet\n",
    "!pip install wget --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97aff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n> Changing the dir to:')\n",
    "%cd /content/lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2de8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n> Download the model')\n",
    "#!curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip\n",
    "!unzip big-lama.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "canvas_html = \"\"\"\n",
    "<style>\n",
    ".button {\n",
    "  background-color: #4CAF50;\n",
    "  border: none;\n",
    "  color: white;\n",
    "  padding: 15px 32px;\n",
    "  text-align: center;\n",
    "  text-decoration: none;\n",
    "  display: inline-block;\n",
    "  font-size: 16px;\n",
    "  margin: 4px 2px;\n",
    "  cursor: pointer;\n",
    "}\n",
    "</style>\n",
    "<canvas1 width=%d height=%d>\n",
    "</canvas1>\n",
    "<canvas width=%d height=%d>\n",
    "</canvas>\n",
    "\n",
    "<button class=\"button\">Finish</button>\n",
    "<script>\n",
    "var canvas = document.querySelector('canvas')\n",
    "var ctx = canvas.getContext('2d')\n",
    "\n",
    "var canvas1 = document.querySelector('canvas1')\n",
    "var ctx1 = canvas.getContext('2d')\n",
    "\n",
    "\n",
    "ctx.strokeStyle = 'red';\n",
    "\n",
    "var img = new Image();\n",
    "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
    "console.log(img)\n",
    "img.onload = function() {\n",
    "  ctx1.drawImage(img, 0, 0);\n",
    "};\n",
    "img.crossOrigin = 'Anonymous';\n",
    "\n",
    "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "\n",
    "ctx.lineWidth = %d\n",
    "var button = document.querySelector('button')\n",
    "var mouse = {x: 0, y: 0}\n",
    "\n",
    "canvas.addEventListener('mousemove', function(e) {\n",
    "  mouse.x = e.pageX - this.offsetLeft\n",
    "  mouse.y = e.pageY - this.offsetTop\n",
    "})\n",
    "canvas.onmousedown = ()=>{\n",
    "  ctx.beginPath()\n",
    "  ctx.moveTo(mouse.x, mouse.y)\n",
    "  canvas.addEventListener('mousemove', onPaint)\n",
    "}\n",
    "canvas.onmouseup = ()=>{\n",
    "  canvas.removeEventListener('mousemove', onPaint)\n",
    "}\n",
    "var onPaint = ()=>{\n",
    "  ctx.lineTo(mouse.x, mouse.y)\n",
    "  ctx.stroke()\n",
    "}\n",
    "\n",
    "var data = new Promise(resolve=>{\n",
    "  button.onclick = ()=>{\n",
    "    resolve(canvas.toDataURL('image/png'))\n",
    "  }\n",
    "})\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
    "    display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
    "    data = eval_js(\"data\")\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db253dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pragyanjagadev/Documents/lama'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc70e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, os\n",
    "from IPython.display import HTML, Image\n",
    "from base64 import b64decode\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wget\n",
    "from shutil import copyfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-colab\n",
    "#!pip install --upgrade google-colab\n",
    "from google.colab.output import eval_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'https://ic.pics.livejournal.com/mostovoy/28566193/1224276/1224276_original.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac03d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba2fa27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use /Users/pragyanjagadev/Documents/lama/with_points.png for inpainting\n",
      "Will use /Users/pragyanjagadev/Documents/lama/with_mask.png for inpainting\n"
     ]
    }
   ],
   "source": [
    "fname = cwd + '/with_points.png'\n",
    "mask_path = cwd + '/with_mask.png'\n",
    "\n",
    "#@title Draw a Mask, Press Finish, Wait for Inpainting\n",
    "\n",
    "image64 = base64.b64encode(open(fname, 'rb').read())\n",
    "image64 = image64.decode('utf-8')\n",
    "\n",
    "print(f'Will use {fname} for inpainting')\n",
    "img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
    "\n",
    "print(f'Will use {mask_path} for inpainting')\n",
    "mask = np.array(plt.imread(f'{mask_path}')[:,:,:3])\n",
    "\n",
    "\n",
    "\n",
    "#with_mask = np.array(plt.imread(mask)[:,:,:3])\n",
    "#mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Show a masked image and save a mask\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.subplot(131)\n",
    "\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('mask')\n",
    "#plt.imsave(\"tesr_mask.png\",mask, cmap='gray')\n",
    "\n",
    "plt.subplot(132)\n",
    "img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('img')\n",
    "\n",
    "plt.subplot(133)\n",
    "img = np.array((1-mask.reshape(mask.shape[0], mask.shape[1], -1))*plt.imread(fname)[:,:,:3])\n",
    "_=plt.imshow(img)\n",
    "_=plt.axis('off')\n",
    "_=plt.title('img * mask')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update -n base -c defaults conda\n",
    "#!pip uninstall torch -y\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conda environment\n",
    "conda create -n cuda_venv conda create -n tensorflow python=3.5\n",
    "conda activate cuda_venv\n",
    "\n",
    "# Install pytorch following commands from https://pytorch.org/get-started/locally/ \n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "virtualenv -p python3 envname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa97c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.8.0 torchvision==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830f126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inpainting\n",
      "Traceback (most recent call last):\n",
      "  File \"bin/predict.py\", line 14, in <module>\n",
      "    from saicinpainting.evaluation.utils import move_to_device\n",
      "  File \"/Users/pragyanjagadev/Documents/lama/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n",
      "    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n",
      "  File \"/Users/pragyanjagadev/Documents/lama/saicinpainting/evaluation/losses/base_loss.py\", line 15, in <module>\n",
      "    from .lpips import PerceptualLoss\n",
      "  File \"/Users/pragyanjagadev/Documents/lama/saicinpainting/evaluation/losses/lpips.py\", line 15, in <module>\n",
      "    from saicinpainting.utils import get_shape\n",
      "  File \"/Users/pragyanjagadev/Documents/lama/saicinpainting/utils.py\", line 12, in <module>\n",
      "    from pytorch_lightning import seed_everything\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/__init__.py\", line 25, in <module>\n",
      "    from lightning_fabric.utilities.seed import seed_everything  # noqa: E402\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/__init__.py\", line 29, in <module>\n",
      "    from lightning_fabric.fabric import Fabric  # noqa: E402\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/fabric.py\", line 32, in <module>\n",
      "    from lightning_fabric.plugins import Precision  # avoid circular imports: # isort: split\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/plugins/__init__.py\", line 18, in <module>\n",
      "    from lightning_fabric.plugins.precision.amp import MixedPrecision\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/plugins/precision/__init__.py\", line 14, in <module>\n",
      "    from lightning_fabric.plugins.precision.amp import MixedPrecision\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/plugins/precision/amp.py\", line 29, in <module>\n",
      "    class MixedPrecision(Precision):\n",
      "  File \"/Users/pragyanjagadev/opt/anaconda3/lib/python3.8/site-packages/lightning_fabric/plugins/precision/amp.py\", line 102, in MixedPrecision\n",
      "    def _autocast_context_manager(self) -> torch.autocast:\n",
      "AttributeError: module 'torch' has no attribute 'autocast'\n"
     ]
    }
   ],
   "source": [
    "print('Run inpainting')\n",
    "if '.jpeg' in fname:\n",
    "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output dataset.img_suffix=.jpeg > /dev/null\n",
    "elif '.jpg' in fname:\n",
    "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.jpg > /dev/null\n",
    "elif '.png' in fname:\n",
    "    !PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output  dataset.img_suffix=.png > /dev/null\n",
    "else:\n",
    "    print(f'Error: unknown suffix .{fname.split(\".\")[-1]} use [.png, .jpeg, .jpg]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hydra-core --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a04929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.imshow(plt.imread(f\"/content/output/{fname.split('.')[1].split('/')[2]}_mask.png\"))\n",
    "_=plt.axis('off')\n",
    "_=plt.title('inpainting result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e474c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
